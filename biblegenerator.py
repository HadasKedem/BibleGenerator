# -*- coding: utf-8 -*-
"""bibleGenerator

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bS3GP26uAwB3zLTEWCdzbVm3Bf1VLXWv
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from random import sample
import random
import pandas as pd

!git clone https://github.com/HadasKedem/BibleGenerator.git
data = pd.read_csv('BibleGenerator/t_bbe.csv')


del data['id']
del data['c']
del data['v']
del data['b']
data.head()

print(data)

EOS_TOKEN = 0
MAX_SEQ_LEN = 100

class Vocab:
    def __init__(self):
        self.char2id = {}
        self.id2char = {}
        self.n_chars = 1
        
    def index_sentence(self, sentence):
      indexes = [self.index_char(c) for c in sentence]
      indexes.append(EOS_TOKEN)
      return indexes
    
    def index_char(self, c):
        if c not in self.char2id:
            self.char2id[c] = self.n_chars
            self.id2char[self.n_chars] = c
            self.n_chars += 1
        return self.char2id[c]
            
            
def prepare_data(data):
    vocab = Vocab()
    data_sequences = []
    for i in data.index:
        if len(data['t'][i]) <= MAX_SEQ_LEN:
            indexes = vocab.index_sentence(data['t'][i])
            data_sequences.append(indexes)
    print(data_sequences)
    return data_sequences, vocab

data_sequences, vocab = prepare_data(data)

class TextGen(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, n_layers):
        super(TextGen, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)
        self.out = nn.Linear(hidden_size, output_size)
    
    def forward(self, char_input, hidden):
        embedded = self.embedding(char_input).view(1, 1, -1)
        output, hidden = self.lstm(embedded, hidden)
        output = self.out(output.view(1, -1))
        return output, hidden
    
    def init_hidden(self):
        return (torch.zeros(self.n_layers, 1, self.hidden_size).cuda(),
                torch.zeros(self.n_layers, 1, self.hidden_size).cuda())

hidden_size = 800
n_layers = 1

# Initialize the model
model = TextGen(vocab.n_chars, hidden_size, vocab.n_chars, n_layers).cuda()

# Loss function
criterion = nn.CrossEntropyLoss()

# Optimizer (ADAM is a fancy version of SGD)
optimizer = optim.Adam(model.parameters(), lr=0.0001)

def generate(model, vocab, start_string, temperature, max_len):
    '''
    This function gets a trained model and vocab and generates a random string
    using the model, seeded with the start_string string.
    The temparature value is used to generate a more diverse output (high value),
    then a conservative one (low value).
    '''
    str = start_string
    sequence = vocab.index_sentence(start_string)
        
    # remove the EOS, we don't need it for generation
    sequence = sequence[:-1]
    
    hidden = model.init_hidden()
    sequence_ten = torch.LongTensor(sequence).cuda()
    print("1")
    for i in range(len(sequence_ten) - 1):
      _, hidden = model(sequence_ten[i], hidden)
      
    output, hidden = model(sequence_ten[-1], hidden) 
    out_dist = output.view(-1).div(temperature).exp()
    print("1")
    new_c = vocab.id2char[torch.multinomial(out_dist, 1)[0].item()]
    str += new_c
    for i in range(max_len):
        new_c_var = torch.LongTensor([vocab.index_char(new_c)]).cuda()
        output, hidden = model(new_c_var, hidden)
        out_dist = output.view(-1).div(temperature).exp()
        char_id = torch.multinomial(out_dist, 1)[0].item()
        if char_id == EOS_TOKEN:
            return str
        new_c = vocab.id2char[char_id]
        str += new_c
    return str

"""Generating texts before training the model:"""

for i in range(10):
    print(generate(model, vocab, 'h', 1, 200))

n_epochs = 10
print_every = 100
loss = 0
for e in range(1, n_epochs + 1):
    data_sequences_shuff = sample(data_sequences, len(data_sequences))
    
    for counter, sequence in enumerate(data_sequences_shuff):
      optimizer.zero_grad()
    
      seq_len = len(sequence)
      sequence_tensor = torch.LongTensor(sequence).cuda()
      
      sentence_loss = 0
      hidden = model.init_hidden()
      
      for i in range(seq_len - 1):
        output, hidden = model(sequence_tensor[i], hidden)
        sentence_loss += criterion(output.view(-1).unsqueeze(0), sequence_tensor[i + 1].unsqueeze(0))
    
      sentence_loss.backward()
      
      optimizer.step()
      
      loss += (sentence_loss.item() / seq_len)
      
      if counter % print_every == 0:
          loss = loss / print_every
          print('Epoch %d, %d/%d, Current Loss = %.4f' % (e, counter, len(data_sequences_shuff), loss))
          loss = 0

"""Generating text after 40 epochs of training"""

for i in range(10):
    print(generate(model, vocab, 'and', 1, 500))

pip install transformers
import matplotlib.pyplot as plt
from torchtext.data import Field, TabularDataset, BucketIterator, Iterator

# Models

import torch.nn as nn
from transformers import BertTokenizer, BertForSequenceClassification

# Training

import torch.optim as optim

# Evaluation

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

